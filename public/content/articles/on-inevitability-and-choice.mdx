---
title: On Inevitability and Choice in AI Futures
slug: on-inevitability-and-choice
tags:
  - Thought Leadership
  - AI Futures
excerpt: Thoughts on how we talk about inevitability in AI, and why preserving a sense of choice matters for teams building real products.
thumbnail: /images/articles/ai-futures-placeholder.jpg
---

When teams talk about AI, *inevitability* shows up quickly.

It sounds like:

- “This is where the industry is going.”
- “We have to do something with AI.”
- “If we don’t move now, we’ll be left behind.”

There’s a grain of truth here—tooling is moving fast, and ignoring it entirely is a choice in itself. But inevitability is a poor design constraint. It flattens nuance and hides the more interesting question:

> Given who we are, what we value, and who we serve, **what kind of AI future do we actually want to participate in?**

Once you ask that, a few things become clearer:

1. Not every capability is relevant to your product.
2. Not every automation is worth the operational risk it introduces.
3. “Doing something with AI” is not the same as building a durable advantage.

In practice, this often means reframing the brief:

- From *“Where can we sprinkle AI?”*  
  to *“Where do humans consistently struggle, and what would it mean to support them better?”*

- From *“How do we replace X?”*  
  to *“How do we give skilled people better tools so they can do more of the work only they can do?”*

This is where I spend most of my time with teams: mapping the real constraints, naming the risks out loud, and designing interventions that feel grounded rather than breathless. The outcome isn’t just a roadmap; it’s a shared sense of why this particular future is worth building.





